{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from src.parallel_score.parallel_score import process_batch\n",
    "\n",
    "def assign_partition_key(df, n_partitions):\n",
    "    df['partition_key'] = df.index % n_partitions\n",
    "    return df\n",
    "\n",
    "def print_statistics(df, key):\n",
    "    print(f'Source size: {df.shape}')\n",
    "\n",
    "    # Find number of rows by LOC_ID\n",
    "    data_by_key_df = df[key].value_counts()\n",
    "\n",
    "    total = 0\n",
    "    for loc_id in data_by_key_df.index:\n",
    "        total += data_by_key_df[loc_id]\n",
    "        # print(f'LOC_ID: {loc_id} - {data_by_key_df[loc_id]}')\n",
    "\n",
    "    print(f'Total: {total} records, {len(data_by_key_df)} {key}s')\n",
    "    return data_by_key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_df = pd.read_csv('data\\source_comments.csv')\n",
    "df_partitioned = assign_partition_key(src_df, 20)\n",
    "df_partitioned.to_csv('data\\source_comments_partitioned.csv', index=False)\n",
    "print('====Partition statistics====')\n",
    "print_statistics(df_partitioned, 'partition_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = process_batch(df_partitioned)\n",
    "\n",
    "out_data.to_csv(os.getcwd() + '\\\\results\\local_resulys.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source_df = pd.read_csv('data\\source_comments_partitioned.csv')\n",
    "source_non_empty_df = source_df.dropna(subset=['FEEDBACK'])\n",
    "r1_df = pd.read_csv('results\\consolidated_results_3.csv')\n",
    "r1_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add partition keys 1-20 to the input data such that each partition has equal number of rows\n",
    "# source_non_empty_df['partition_key'] = source_non_empty_df.index % 15 + 1\n",
    "# source_non_empty_df.to_csv('data\\source_comments_partitioned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('====Source statistics====')\n",
    "print_statistics(source_non_empty_df, 'partition_key')\n",
    "\n",
    "print('====Results statistics====')\n",
    "x1 = print_statistics(r1_df, 'COMPLETION')\n",
    "\n",
    "# sort by count and print first 5\n",
    "x1.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest = r1_df['START_TIME'].min()\n",
    "latest = r1_df['END_TIME'].max()\n",
    "total_time = latest - earliest\n",
    "size = r1_df.shape[0]\n",
    "processing_rate = size / total_time\n",
    "total_time_minutes = total_time / 60\n",
    "\n",
    "# Calculate processing time per record\n",
    "r1_df['PROCESSING_TIME'] = r1_df['END_TIME'] - r1_df['START_TIME']\n",
    "average_processing_time = r1_df['PROCESSING_TIME'].mean()\n",
    "\n",
    "retry_count = r1_df['RETRY'].value_counts()\n",
    "\n",
    "print(f'Records Processed: {size}')\n",
    "print(f'Processing time: {total_time_minutes} minutes')\n",
    "print(f'Processing rate: {processing_rate} records per second')\n",
    "print(f'Average processing time: {average_processing_time} seconds')\n",
    "\n",
    "print(f'=====Retries===')\n",
    "for retry in retry_count.index:\n",
    "    print(f'{retry}: {retry_count[retry]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_df = r1_df[r1_df['COMPLETION'] == 'Ratelimit Error']\n",
    "print(f'Ratelimit: {errors_df.shape[0]}')\n",
    "\n",
    "errors_df = r1_df[r1_df['COMPLETION'] == 'Error']\n",
    "print(f'Errors: {errors_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count columns where Completion is not 'None'\n",
    "non_none_df = r1_df[r1_df['COMPLETION'] != None]\n",
    "print(f'Non-None: {non_none_df.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
