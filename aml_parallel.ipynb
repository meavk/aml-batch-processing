{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orange juice sales prediction example \\[Parallel job\\] \\[SDK example\\]\n",
    "## Key notes for this example\n",
    "- How to use **parallel job** for **many model training** scenario.\n",
    "- How to use parallel job **run_function** task with predefined **entry_script**.\n",
    "- How to pre-cook data into **mltable with partition setting**.\n",
    "- How to use **mltable** with **tabular data** as the **input of parallel job**.\n",
    "- How to use **partition_keys** in parallel job to consume data with partitions. \n",
    "- How to use **error_threshold** with **empty returns** to ignore checking failed items in mini-batch.\n",
    "- How to use other parallel job settings:\n",
    "  - mini_batch_error_threshold\n",
    "  - environment_variables\n",
    "\n",
    "To get the same example with CLI + Yaml experience, please refer to: [link](../../../../../cli/jobs/parallel/1a_oj_sales_prediction/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673838207609
    },
    "name": "required-library"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input, Output, load_component\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml.entities import Environment, ResourceConfiguration\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673838211196
    },
    "name": "credential"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     credential = DefaultAzureCredential()\n",
    "#     # Check if given credential can get token successfully.\n",
    "#     credential.get_token(\"https://management.azure.com/.default\")\n",
    "# except Exception as ex:\n",
    "#     # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "#     credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673858642606
    },
    "name": "workspace"
   },
   "outputs": [],
   "source": [
    "# Get a handle to workspace\n",
    "subscription_id = \"34397f45-ab9c-45d4-8a0c-5d2b2750ed3d\"\n",
    "resource_group = \"meavk-aml\"\n",
    "workspace = \"meavk-aml-spark\"\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")\n",
    "\n",
    "# Retrieve an already attached Azure Machine Learning Compute.\n",
    "cpu_compute_target = \"batch-cluster\"\n",
    "print(ml_client.compute.get(cpu_compute_target))\n",
    "# gpu_compute_target = \"gpu-cluster\"\n",
    "# print(ml_client.compute.get(gpu_compute_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define components and jobs in pipeline\n",
    "\n",
    "## 2.1 Load existing command component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673860079582
    },
    "name": "load-from-yaml"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load existing command component to partition the single csv data to mltable.\n",
    "path = os.getcwd() + '\\src\\partition_data\\partition_data.yml'\n",
    "print(path)\n",
    "partition_data = load_component(source=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Declare parallel job by `parallel_run_function`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673860081288
    },
    "name": "parallel-job-for-file-data"
   },
   "outputs": [],
   "source": [
    "# Declare parallel job with run_function task\n",
    "many_model_training_with_partition_keys = parallel_run_function(\n",
    "    name=\"theme_and_sentiments_extraction_parallel\",\n",
    "    display_name=\"Extract Themes and Sentiments in parallel\",\n",
    "    description=\"Parallel job to extract themes and sentiments\",\n",
    "    inputs=dict(\n",
    "        data_source=Input(\n",
    "            type=AssetTypes.MLTABLE,\n",
    "            description=\"Input mltable with predefined partition format.\",\n",
    "            mode=InputOutputModes.DIRECT,  # [Important] To use 'partition_keys', input MLTable is required to use 'direct' mode.\n",
    "        )\n",
    "    ),\n",
    "    outputs=dict(\n",
    "        job_output_folder=Output(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            mode=InputOutputModes.RW_MOUNT,\n",
    "        ),\n",
    "    ),\n",
    "    input_data=\"${{inputs.data_source}}\",  # Define which input data will be splitted into mini-batches\n",
    "    partition_keys=[\n",
    "        \"partition_key\"\n",
    "    ],  # Use 'partition_keys' as the data division method. This method requires MLTable input with partition setting pre-defined in MLTable artifact.\n",
    "    instance_count=2,  # Use 2 nodes from compute cluster to run this parallel job.\n",
    "    # max_concurrency_per_instance=1,  # Create 2 worker processors in each compute node to execute mini-batches.\n",
    "    error_threshold=-1,  # Monitor the failures of item processed by the gap between mini-batch input count and returns. 'Many model training' scenario doesn't fit this setting and '-1' means ignore counting failure items by mini-batch returns.\n",
    "    mini_batch_error_threshold=5,  # Monitor the failed mini-batch by exception, time out, or null return. When failed mini-batch count is higher than this setting, the parallel job will be marked as 'failed'.\n",
    "    retry_settings=dict(\n",
    "        max_retries=2,  # Define how many retries when mini-batch execution is failed by exception, time out, or null return.\n",
    "        timeout=3600,  # Define the timeout in second for each mini-batch execution.\n",
    "    ),\n",
    "    logging_level=\"DEBUG\",\n",
    "    task=RunFunction(\n",
    "        code= os.getcwd() + '\\src\\parallel_score\\\\',\n",
    "        entry_script=\"parallel_score.py\",\n",
    "        environment=Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    "            conda_file= os.getcwd() + '\\src\\parallel_score\\conda.yaml',\n",
    "        ),\n",
    "        program_arguments=\"--job_output_folder ${{outputs.job_output_folder}} \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673860087207
    },
    "name": "build-pipeline"
   },
   "outputs": [],
   "source": [
    "# Declare the overall input of the job.\n",
    "data_path = os.getcwd() + '\\data\\source_comments_partitioned.csv'\n",
    "\n",
    "input_data = Input(\n",
    "    path=data_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    mode=InputOutputModes.RO_MOUNT,\n",
    ")\n",
    "\n",
    "# Declare pipeline structure.\n",
    "@pipeline(\n",
    "    display_name=\"Themes and sentiments generation pipeline\",\n",
    ")\n",
    "def partition_job_in_pipeline(\n",
    "    pipeline_input_data,\n",
    "):\n",
    "    # Declare 1st data partition command job.\n",
    "    partition_job = partition_data(\n",
    "        data_source=pipeline_input_data,\n",
    "        partition_keys=\"partition_key\",\n",
    "    )\n",
    "\n",
    "    # Declare 2nd parallel model training job.\n",
    "    parallel_score = many_model_training_with_partition_keys(\n",
    "        data_source=partition_job.outputs.tabular_output_data\n",
    "    )\n",
    "\n",
    "    # User could override parallel job run-level property when invoke that parallel job/component in pipeline.\n",
    "    parallel_score.resources.instance_count = 4\n",
    "    parallel_score.max_concurrency_per_instance = 5\n",
    "    parallel_score.mini_batch_error_threshold = 10\n",
    "    parallel_score.outputs.job_output_folder.path = \"azureml://datastores/${{default_datastore}}/paths/${{name}}/parallel_job_output/\"\n",
    "\n",
    "\n",
    "# Create pipeline instance\n",
    "my_job = partition_job_in_pipeline(\n",
    "    pipeline_input_data=input_data,\n",
    ")\n",
    "\n",
    "# Set pipeline level compute\n",
    "my_job.settings.default_compute = \"batch-cluster\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673860094391
    }
   },
   "outputs": [],
   "source": [
    "print(my_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_client = MLClient(\n",
    "#     InteractiveBrowserCredential(), subscription_id, resource_group, workspace\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1673858666260
    },
    "name": "submit-pipeline"
   },
   "outputs": [],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    my_job,\n",
    "    experiment_name=\"score-parallel-job\",\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "stream-pipeline"
   },
   "outputs": [],
   "source": [
    "# wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  }
 ],
 "metadata": {
  "description": {
   "description": "Create pipeline with parallel node to do batch inference"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1648a01f415a15976454e88ab551f1eeb39d06522c1fdad5697f49923f4699e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
